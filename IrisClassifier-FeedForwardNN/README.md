# NumPy Neural Network & PyTorch MLP â€” From-Scratch Implementation

This project showcases a complete feed-forward neural network built **from scratch using NumPy**, as well as a **PyTorch-based MLP** trained on the FashionMNIST dataset. It was developed as part of **ECE421: Introduction to Machine Learning (Fall 2024)** at the University of Toronto.

## ğŸš€ Key Features

### ğŸ”· Part 1: Custom NumPy Feed-Forward Neural Network
- Fully Connected Layers with ReLU and Softmax
- Batch training with Cross Entropy Loss
- Gradient computation via backpropagation
- Modular architecture with activations, layers, loss, and optimizer
- Visualization of training and validation accuracy/loss
- Trainable on the Iris dataset

### ğŸ”¶ Part 2: PyTorch MLP on FashionMNIST
- Uses `nn.Linear`, `nn.ReLU`, and `nn.CrossEntropyLoss`
- Achieves â‰¥82% validation accuracy
- Includes training and validation loss plots
- Implemented in `PA3.ipynb` (Colab notebook)

## ğŸ“ File Structure
- `activations.py` â€“ Implements ReLU, Softmax, Sigmoid, etc.
- `layers.py` â€“ Fully connected layer with forward/backward logic
- `loss.py` â€“ Cross-entropy loss with backprop
- `model.py` â€“ Neural network model class (training, forward, backward)
- `util.py` â€“ Dataset loader, logging, preprocessing tools
- `PA3.ipynb` â€“ PyTorch implementation on FashionMNIST (Colab notebook)

## ğŸ§ª Run Instructions (NumPy Part)
```bash
# Make sure all dependencies are installed
pip install -r requirements.txt

# Then, you can run training and testing via model.py and util.py (or via provided notebook)
```

## ğŸ“ Author
Developed by Najma Sultani as part of ECE421 â€“ Introduction to Machine Learning at University of Toronto, Fall 2024
